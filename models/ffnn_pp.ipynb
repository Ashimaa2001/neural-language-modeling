{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "!pip install contractions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "import random\n",
    "import re\n",
    "import nltk\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "import contractions\n",
    "\n",
    "# Ensure the required resources are downloaded\n",
    "nltk.download('punkt')\n",
    "nltk.download('punkt_tab')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PreProcessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Preprocessing function\n",
    "def preprocess_text(text):\n",
    "    # Remove unnecessary symbols\n",
    "    text = re.sub(r'[\\*\\(\\)\\[\\]{}]', '', text)  # Remove *, (, ), [, ], {, }\n",
    "    text = re.sub(r'\\s+', ' ', text).strip()  # Normalize spaces\n",
    "\n",
    "    text = text.lower()\n",
    "    text = contractions.fix(text)  # Expand contractions\n",
    "\n",
    "    # Remove Project Gutenberg boilerplate and metadata\n",
    "    text = re.sub(r'\\*\\*\\*.*?\\*\\*\\*', '', text, flags=re.DOTALL)  # Remove START/END OF GUTENBERG EBOOK\n",
    "    text = re.sub(r'(title|author|release date|language|e-text|prepared by)[^:]*:.*?(\\n|$)', '', text)\n",
    "    text = re.sub(r'(project gutenberg license).*?(\\n|$)', '', text, flags=re.DOTALL)\n",
    "\n",
    "    # Replace patterns with contextual tags\n",
    "    text = re.sub(r'http\\S+|www\\S+|ftp\\S+', '<URL>', text)  # URLs\n",
    "    text = re.sub(r'\\b(\\d{1,2}[:.]\\d{2}\\s?[ap][m]|\\b(?:[a-z]+ \\d{1,2},? \\d{4})\\b)', '<TIME>',text)  # Times and dates\n",
    "    text = re.sub(r'\\d+%', '<PERCENT>', text)  # Percentages\n",
    "    text = re.sub(r'\\d+\\s?(?:years?|yrs?)\\s?old', '<AGE>', text)  # Ages\n",
    "    text = re.sub(r'@\\w+', '<USER_MENTION>', text)  # Mentions\n",
    "    text = re.sub(r'#\\w+', '<TOPIC_HASHTAG>', text)  # Hashtags\n",
    "\n",
    "    # Handle ordinals and superscripts\n",
    "    text = re.sub(r'(\\d+\\^?(?:st|nd|rd|th))', '<ORDINAL>', text)  # Ordinals like 1^st, 2^nd\n",
    "    text = re.sub(r'(\\d\\^st|\\d\\^nd|\\d\\^rd|\\d\\^th)', '<ORDINAL>', text)  # Handle other ordinals\n",
    "\n",
    "    # Handle possessives\n",
    "    text = re.sub(r'\\d+', '<NUM>', text)  # Replace numbers with <NUM> token\n",
    "    text = re.sub(r\"\\b([a-z]+['â€™][sS]?)\\b\", r'\\1', text)\n",
    "\n",
    "    # Handle dates and times\n",
    "    text = re.sub(r'\\b(\\d{1,2} \\w+ \\d{4})\\b', '<DATE>', text)  # Format like 01 January 1813\n",
    "    text = re.sub(r'\\b(\\d{1,2}[:.]\\d{2} [ap][m])\\b', '<TIME>', text)  # Format like 12:30 PM\n",
    "\n",
    "    text = re.sub(r'chapter\\s+[ivxlcdm]+\\.', '', text)\n",
    "\n",
    "    text = contractions.fix(text)  # Expand contractions\n",
    "\n",
    "    text = re.sub(r'[^a-z0-9\\s<>]', '', text)  # Keep only alphanumeric and relevant symbols\n",
    "\n",
    "    return text\n",
    "\n",
    "def load_and_split_data(sentences, test_size=1000, val_size=2000):\n",
    "    random.shuffle(sentences)\n",
    "    test_sentences = sentences[:test_size]\n",
    "    val_sentences = sentences[test_size:test_size + val_size]\n",
    "    train_sentences = sentences[test_size + val_size:]\n",
    "    return train_sentences, val_sentences, test_sentences\n",
    "\n",
    "\n",
    "# Tokenization function\n",
    "def tokenize_text(text, test_size=1000, val_size=2000,split = False):\n",
    "\n",
    "    # Tokenize sentences using NLTK's sent_tokenize to split into sentences\n",
    "    sentences = sent_tokenize(text)\n",
    "    if split:\n",
    "        train_sentences,val_sentences, test_sentences = load_and_split_data(sentences, test_size=1000,val_size=2000)\n",
    "    else:\n",
    "        train_sentences = sentences\n",
    "        val_size = []\n",
    "        test_sentences = []\n",
    "\n",
    "\n",
    "    train_tokenized_sentences = []\n",
    "\n",
    "    for sentence in train_sentences:\n",
    "        # Preprocess text\n",
    "        sentence = preprocess_text(sentence)\n",
    "\n",
    "        # Tokenize each sentence into words\n",
    "        words = sentence.split()\n",
    "\n",
    "        train_tokenized_sentences.append(words)\n",
    "\n",
    "    test_tokenized_sentences = []\n",
    "\n",
    "    for sentence in test_sentences:\n",
    "        # Preprocess text\n",
    "        sentence = preprocess_text(sentence)\n",
    "\n",
    "        # Tokenize each sentence into words\n",
    "        words = sentence.split()\n",
    "\n",
    "        test_tokenized_sentences.append(words)\n",
    "\n",
    "    val_tokenized_sentences = []\n",
    "\n",
    "    for sentence in val_sentences:\n",
    "        # Preprocess text\n",
    "        sentence = preprocess_text(sentence)\n",
    "\n",
    "        # Tokenize each sentence into words\n",
    "        words = sentence.split()\n",
    "\n",
    "        val_tokenized_sentences.append(words)\n",
    "\n",
    "    return train_tokenized_sentences,val_tokenized_sentences,test_tokenized_sentences\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
